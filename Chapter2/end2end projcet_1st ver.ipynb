import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
housing0=pd.read_csv('housing.csv')
housing0.head()
housing0.describe()
housing0.shape
housing0.info()
housing0.hist(bins=50,figsize=(12,8))
# the medium_income has outliers after 5 and most values  are distriputed between 1.5 and 6
# this column will be used for statfied train and test data 
housing0["income_cat"] = pd.cut(housing0["median_income"],bins=[0., 1.5, 3.0, 4.5, 6., np.inf],labels=[1, 2, 3, 4, 5])

housing0["income_cat"].value_counts().sort_index().plot.bar(rot=0, grid=True)
plt.xlabel("Income category")
plt.ylabel("Number of districts")
plt.show()

"""
Creates a new column income_cat by binning median_income into 5 categories (1 to 5).

pd.cut() splits continuous data into intervals or bins.

The bins argument defines the ranges of income:

[0., 1.5) → category 1

[1.5, 3.0) → category 2

[3.0, 4.5) → category 3

[4.5, 6.0) → category 4

[6.0, ∞) → category 5
"""

print(housing0['income_cat']) # cateorized income 
print(housing0.shape)
housing0.shape
housing0.head()
# taking a copy from the data
housing1=housing0.copy()
housing1.head()
# to distribute the income cat into  the data set and test set as original data 

trainData, testData = train_test_split(housing1, test_size=0.2, stratify=housing1["income_cat"], random_state=42)
# measure each category contribution in the test data 
# 3 category --> 3   0.350533 --> 35% contribution
testData['income_cat'].value_counts()/len(testData)
# measure each category contribution in the whole data 
# 4 category --> 4   0.176308 --> 17% contribution
housing1['income_cat'].value_counts()/len(housing1)
## Explore and Visualize the Data to Gain Insights


housing1.plot(kind="scatter", x="longitude", y="latitude", grid=True, alpha=0.5)
plt.show()
housing1.plot(kind="scatter", x="longitude", y="latitude", grid=True,s=housing1["population"] / 100, label="population",c="median_house_value", cmap="jet", colorbar=True,
legend=True, sharex=False, figsize=(10, 7))
plt.show()


#we have droped ocaen data because they are not numerical data 
corr_matrix = housing1.drop(columns=["ocean_proximity",'income_cat'],axis=1).corr()

corr_matrix['median_house_value'].sort_values(ascending=False)

from pandas.plotting import scatter_matrix
attributes = ["median_house_value", "median_income", "total_rooms","housing_median_age"]
scatter_matrix(housing1[attributes], figsize=(12, 8))
plt.show()
## to measure correlation between two features
scatter_matrix(housing1[["total_bedrooms","median_house_value"]],figsize=(8,8))
plt.show()

### from the previous correlation of median_house_value its noteced that there are some  features has nearly  no effect
### so we will do some transformations for it 

housing1["rooms_per_house"]  =  housing1["total_rooms"] / housing1["households"]
housing1["bedrooms_ratio"]   =  housing1["total_bedrooms"] / housing1["total_rooms"]
housing1["people_per_house"] =  housing1["population"] / housing1["households"]
corr_matrix['median_house_value'].sort_values(ascending=False)
## Prepare the Data for Machine Learning Algorithms p96 pdf
housing2 = trainData.drop("median_house_value", axis=1)
housing_labels = trainData["median_house_value"].copy()

### Clean the Data
housing2.isnull().sum()
## fill the null values
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy="median")
import numpy as np
housing2_num=housing2.select_dtypes(include=[np.number]) # select just numerical data
X=imputer.fit_transform(housing2_num)
X_df=pd.DataFrame(X,columns=housing2_num.columns,index=housing2_num.index)
X_df
X_df.isnull().sum()
ocean=housing2[['ocean_proximity']].head(8) #page99
### OrdinalEncoder:
Converts each category into a single integer.
Example: ["red", "blue", "green"] → [0, 1, 2].
Output is a single column of integers.

### OneHotEncoder:
Converts each category into a binary vector (one-hot encoding).
Example: ["red", "blue", "green"] → [[1, 0, 0], [0, 1, 0], [0, 0, 1]].
Output is multiple binary columns (one per category).


# When to Use Which?
### Use OrdinalEncoder when:
The categorical data has an inherent order (e.g., ["low", "medium", "high"]).
You’re using tree-based models that can interpret ordinal relationships.
You want to keep the dimensionality low.

### Use OneHotEncoder when:
The categorical data is nominal (no inherent order, e.g., ["red", "blue", "green"]).
You’re using linear models or neural networks that cannot interpret ordinal relationships.
You don’t mind increasing the dimensionality of your dataset.

### OrdinalEncoder
from sklearn.preprocessing import OrdinalEncoder # Converts Categories to Integers (it is not the same with pd.cut that convert !!!numerical cts to sts)
ordEn=OrdinalEncoder()
ordencodded=ordEn.fit_transform(ocean)
ordencodded
ordEn.categories_
### OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
ohE=OneHotEncoder(sparse_output =False) # this parmeter convert the output into an array directly 
onhtencodded=ohE.fit_transform(ocean)
onhtencodded
ohE.categories_
## Feature Scaling and Transformation
### MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
MMsclr=MinMaxScaler(feature_range=(-1,1))
MMscaled=MMsclr.fit_transform(housing2_num)
MMscaled
### StandardScaler
from sklearn.preprocessing import StandardScaler
SSsclr=StandardScaler()
SSscaled=SSsclr.fit_transform(housing2_num)
SSscaled
### RPF p106
from sklearn.metrics.pairwise import rbf_kernel # The Radial Basis Function (RBF) kernel measures similarity based on Euclidean distance.
age_simil_35 = rbf_kernel(housing0[["housing_median_age"]], [[35]], gamma=0.1)
print(age_simil_35)
"""
the output represents the similarity between each value of the housing_median_age feature in the dataset and the value 35,
calculated using the Radial Basis Function (RBF) kern
"""
## How to see the real predictions if we did some transformation on target values
#### 1- Using inverse transform
# if we did something(like scaling) on target values we use sklearn inverse_transform to get the real prediction vlaue 
from sklearn.linear_model import LinearRegression
target_scaler = StandardScaler()
scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())
model = LinearRegression()
model.fit(housing2[["median_income"]], scaled_labels)
some_new_data = housing2[["median_income"]].iloc[:5] # pretend this is new data
scaled_predictions = model.predict(some_new_data)
predictions = target_scaler.inverse_transform(scaled_predictions)

print('Prediction before inverse ' ,scaled_predictions) #scaled
print()
print('predictions after doing inverse transform ',predictions) #non scaled
#### 2 - TransformedTargetRegressor
# instead doing scaling manual then inverse it again there is a sklearn module do this 
from sklearn.compose import TransformedTargetRegressor
model = TransformedTargetRegressor(LinearRegression(),transformer=StandardScaler())
model.fit(housing2[["median_income"]], housing_labels)
predictions = model.predict(some_new_data)
print(predictions)
#### 3 - FunctionTransformer
# the previous transfomer converet only scaled data but if we need custom transfomration
from sklearn.preprocessing import FunctionTransformer
log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
log_pop = log_transformer.transform(housing2[["population"]])
log_pop

# Plot histogram to compare distributions
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Original data distribution
axes[0].hist(housing2["population"], bins=20, color='blue', alpha=0.7)
axes[0].set_title("Original Population Distribution")
axes[0].set_xlabel("Population")
axes[0].set_ylabel("Frequency")

# Log-transformed data distribution
axes[1].hist(log_pop, bins=20, color='c', alpha=0.7)
axes[1].set_title("Log-Transformed Population Distribution")
axes[1].set_xlabel("Log(Population)")
axes[1].set_ylabel("Frequency")

plt.show()

rbf_transformer = FunctionTransformer(rbf_kernel,kw_args=dict(Y=[[35.]], gamma=0.1))
age_simil_35 = rbf_transformer.transform(housing2[["housing_median_age"]])
age_simil_35
# The Radial Basis Function (RBF) kernel measures similarity based on Euclidean distance So it -> has no inverse 
sf_coords = 37.7749, -122.41  # Latitude and longitude of San Francisco
sf_transformer = FunctionTransformer(rbf_kernel,kw_args=dict(Y=[sf_coords], gamma=0.1))  # Y is the reference point (San Francisco))
sf_simil = sf_transformer.transform(housing2[["latitude", "longitude"]])
sf_simil
# Custom Transformation p(107,79)
### 1- Transfomring features that not requires (fit , transform)
housing3=pd.read_csv('housing.csv')
housing3.info()
## Examlpe One
## using FunctionTransformer 
from sklearn.preprocessing import FunctionTransformer
log_trans=FunctionTransformer(np.log,inverse_func=np.exp)
print(housing3['population']) # before 
print()
print(log_trans.transform(housing3[['population']])) # after 
## Example 2 meesure similarity between housing_meadian_age and 35
from sklearn.metrics.pairwise import rbf_kernel
rbf_transformer = FunctionTransformer(rbf_kernel,kw_args=dict(Y=[[35.]], gamma=0.1))
age_simil_35 = rbf_transformer.transform(housing3[["housing_median_age"]])
print(housing3['housing_median_age']) ## before
print('\n')
print(age_simil_35) #after 
#### Example 2 <br>
![image.png](attachment:image.png)
## Example 2 calculates how geographically similar each house's location is to San Francisco.
sf_coords = 37.7749, -122.41  #  San Francisco. location
sf_transformer = FunctionTransformer(rbf_kernel,kw_args=dict(Y=[sf_coords], gamma=0.1))
sf_simil = sf_transformer.transform(housing3[["latitude", "longitude"]])
sf_simil

## Example 3 calculate ratio between two columns (first and second columns)
ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])
ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))
# Second Method Class method
when we need to make a transformation but we need to train on (fit and transform) we use class methos where fit and transform  <br>
are two methods of this class
from sklearn.pipeline import Pipeline
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):  # No *args or **kwargs needed!
        self.with_mean = with_mean

    def fit(self, X, y=None):  # 'y' is required by sklearn API, but not used
        X = check_array(X)  # Ensures X is a valid array with finite float values
        self.mean_ = X.mean(axis=0)  # Compute mean for each feature
        self.scale_ = X.std(axis=0)  # Compute standard deviation for each feature
        self.n_features_in_ = X.shape[1]  # Store the number of features
        return self  # Always return self!

    def transform(self, X):
        check_is_fitted(self)  # Ensures 'fit()' was called before 'transform()'
        X = check_array(X)  # Validate input data
        assert self.n_features_in_ == X.shape[1]  # Ensure same feature count

        if self.with_mean:
            X = X - self.mean_  # Subtract the mean if 'with_mean=True'

        return X / self.scale_  # Scale by standard deviation
    
########### exMPLE ########
# Step 1: Create Sample Data
X = np.array([[10, 200],  [20, 300], [30, 400]])

print("Original Data:")
print(X)

# Step 2: Initialize and Fit the Transformer
scaler = StandardScalerClone(with_mean=True)
scaler.fit(X)

# Step 3: Transform Data
X_transformed = scaler.transform(X)
print("\nStandardized Data:")
print(X_transformed)

# Step 4: Check Mean and Standard Deviation
print("\nComputed Mean:")
print(scaler.mean_)

print("\nComputed Standard Deviation:")
print(scaler.scale_)

# Step 5: Using in a Pipeline
pipeline = Pipeline([ ('scaler', StandardScalerClone())])

# Fit and transform using the pipeline
X_pipeline_transformed = pipeline.fit_transform(X)
print("\nStandardized Data from Pipeline:")
print(X_pipeline_transformed)
# Pipeline In sklearn
#### when we want to do some operation on data (pipeline) we can do them separately or using ( pipeline or make_pipeline )
##### 1- pipeline 
# Enable diagram display
from sklearn import set_config
set_config(display="diagram")

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler


pipeline = Pipeline([("impute", SimpleImputer(strategy="median")),("standardize", StandardScaler()),]) # making imputing and standarizing
print(pipeline.fit_transform(housing3[['median_house_value']])) ## applying pipeline on median_house_value  (impute then StandarScale)
pipeline # show the diagram of pipeline 
### 2- make_pipeline  (same as pipeline but has no names )
from sklearn.pipeline import make_pipeline
mk_pip=make_pipeline(SimpleImputer(strategy='mean'),StandardScaler())
print(mk_pip.fit_transform(housing3[['median_house_value']]))
mk_pip

## pip line can be considered as a list 
print(mk_pip[0])
print(pipeline[1])
### ColumnTransformer --> is used to implement a pipeline for different datatypes of columns
from sklearn.compose import ColumnTransformer
num_attribs = ["longitude", "latitude", "housing_median_age", "total_rooms","total_bedrooms", "population", "households", "median_income"]
cat_attribs = ["ocean_proximity"]

cat_pipeline = make_pipeline(SimpleImputer(strategy="most_frequent"),OneHotEncoder(handle_unknown="ignore")) # pipeline for categorical data
num_pipeline=Pipeline([('imputer',SimpleImputer(strategy='mean'),('scaler',StandardScaler()))])  # pipeline for numerical data
#                                    name  pipelineer   piped columns
preprocessing = ColumnTransformer([("num", num_pipeline, num_attribs),("cat", cat_pipeline, cat_attribs)]) ## distribute pipeline on data 
print(preprocessing)
preprocessing
#### we can ues make_column_selector to auto select specific data types of columns instead make a list of cols names as previous 
# we now can do pipeline to the whole colmn in dataframe without select the columns types they are auto selected by make_column_selector
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_selector ,make_column_transformer


# Now make_column_transformer will work
preprocessing = make_column_transformer(
    (num_pipeline, make_column_selector(dtype_include=np.number)),
    (cat_pipeline, make_column_selector(dtype_include=object)),
)


housing3_processed=preprocessing.fit_transform(housing3)
housing3_processed.shape   

### The full code to prepare our data (combine the previous topics)
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_selector
import numpy as np

""
housing4=pd.read_csv('housing.csv')

# Function to compute column ratio
def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

# Function to name the transformed feature
def ratio_name(function_transformer, feature_names_in):
    return ["ratio"]  # Output feature name

# Pipeline for ratio features
def ratio_pipeline():
    return make_pipeline(
        SimpleImputer(strategy="median"),
        FunctionTransformer(column_ratio, feature_names_out=ratio_name),
        StandardScaler()
    )

# Log transformation pipeline
log_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    FunctionTransformer(np.log, feature_names_out="one-to-one"),
    StandardScaler()
)


# Default numerical pipeline
default_num_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler()
)

# Categorical pipeline (assumed to be defined elsewhere)
cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="most_frequent")),
    ('encoder', OneHotEncoder(handle_unknown="ignore"))
])

# ColumnTransformer combining all transformations
preprocessing = ColumnTransformer([
    ("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"]),
    ("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]),
    ("people_per_house", ratio_pipeline(), ["population", "households"]),
    ("log", log_pipeline, ["total_bedrooms", "total_rooms", "population","households", "median_income"]),
    ("cat", cat_pipeline, make_column_selector(dtype_include=object)),
], remainder=default_num_pipeline)  # Remaining column: "housing_median_age"

# Example usage with housing data
housing_prepared = preprocessing.fit_transform(housing4)
print(housing_prepared.shape)  # Should be (16512, 24)
print(preprocessing.get_feature_names_out())  # Output feature names

#  Select and Train a Model
# 1 - LinearRegression Model
from sklearn.linear_model import LinearRegression
housing5=pd.read_csv('housing.csv')
housing5_features=housing5.drop(columns=['median_house_value'])
housing5_targets=housing5['median_house_value']
model=make_pipeline(preprocessing,LinearRegression())
model.fit(housing5_features,housing5_targets)
# the model prediction of our features
pred1=model.predict(housing5_features)
print(pred1[:5].round(-2))
print(housing5_targets[:5])
from sklearn.metrics import root_mean_squared_error
root_mean_squared_error(pred1,housing5_targets) ## Underfitting
## the range of targets is thousands USD then 71k is larger error so this is Underfitting 
##  it can mean that the features do not provide enough information to make good predictions OR that the model is not powerful enough
# to extract the model itself
model_extracted=model.named_steps['linearregression']
model_extracted
# 2- DecsionTree Model
# we will try decsisioTree model 
from sklearn.tree import DecisionTreeRegressor ,plot_tree
DTreg=make_pipeline(preprocessing,DecisionTreeRegressor(random_state=42))
DTreg.fit(housing5_features,housing5_targets)
pred2=DTreg.predict(housing5_features)
print(pred2[0:5])
print(housing5_targets[0:5])
root_mean_squared_error(housing5_targets,pred2) ## overfit happens

# Extract the trained DecisionTreeRegressor
tree_model = DTreg.named_steps['decisiontreeregressor']
tree_model
## Evaluation by K-Fold

from sklearn.model_selection import cross_val_score
tree_rmses = -cross_val_score(DTreg, housing5_features, housing5_targets,scoring="neg_root_mean_squared_error", cv=10)
tree_rmses
pd.Series(tree_rmses).describe()
##### the mean root mean square = 69142.026988 and still high so we will use RandomForestRegressor
# 3- RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
forest_reg = make_pipeline(preprocessing,RandomForestRegressor(random_state=42))
forest_rmses = -cross_val_score(forest_reg, housing5_features, housing5_targets,scoring="neg_root_mean_squared_error", cv=10)

pd.Series(forest_rmses).describe()
# GridSearch
#### use gridsearch to gest the best parameters for our model 
from sklearn.model_selection import GridSearchCV
full_pipeline = Pipeline([("preprocessing", preprocessing),("random_forest", RandomForestRegressor(random_state=42)),])
## test which number of features of RandomForest Model (we can test another parameters also)
param_grid = [{'random_forest__max_features': [4, 6, 8]},]
grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,scoring='neg_root_mean_squared_error')
grid_search.fit(housing5_features, housing5_targets)
grid_search.best_params_
grid_search.cv_results_
# the detailed results (best chios) which has least mean_test_score 
cv_res = pd.DataFrame(grid_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
[...]  # change column names to fit on this page, and show rmse = -score
cv_res  # note: the 1st column is the row ID
## Rndomized Search 
#### Instead of exhaustively trying all combinations (like Grid Search), Randomized Search picks a random subset of hyperparameter combinations and evaluates them.
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
param_distribs = {'random_forest__max_features': randint(low=2, high=20)}
rnd_search = RandomizedSearchCV(full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,scoring='neg_root_mean_squared_error', random_state=42)
rnd_search.fit(housing5_features, housing5_targets)
## extact the importance of each feature for the model
final_model = rnd_search.best_estimator_  # includes preprocessing
feature_importances = final_model["random_forest"].feature_importances_
list(feature_importances)
## map the score of each feature 
sorted(zip(feature_importances,final_model["preprocessing"].get_feature_names_out()),reverse=True)
# THIS  PART NEED TO BE REVIWED #######
trainData,testData
X_test = testData.drop("median_house_value", axis=1)
y_test = testData["median_house_value"].copy()
final_predictions = final_model.predict(X_test)
final_rmse = root_mean_squared_error(y_test, final_predictions)
print(final_rmse)  # prints 41424.40026462184

#RRRRRRRRRRRRRRRRRRRRRREEEEEEEEEEEEREEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
from scipy import stats
confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,                     
loc=squared_errors.mean(),scale=stats.sem(squared_errors)))
# save the model to use it  later 
import joblib
joblib.dump(final_model, "my_california_housing_model.pkl")
# load the model to use 
loaded_model = joblib.load("my_california_housing_model.pkl")
predictions = loaded_model.predict(X_test)
predictions
import networkx as nx
import matplotlib.pyplot as plt

# Create a graph
G = nx.Graph()
G.add_edges_from([(1, 2), (2, 3), (3, 1), (3, 4),(3,5),(5,1)])

nx.draw(G, with_labels=True)
plt.show()

